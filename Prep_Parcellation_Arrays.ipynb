{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the underlying data and subjects splits ect... will be prepared for each each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import nibabel.freesurfer.io as io\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from ROIs import get_X\n",
    "from Individual import Existing_Surf_Individual\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nilearn import datasets, surface\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different tasks are saved in folders sst_raw_data, nback_raw_data, ect..., with each contrast saved individually stacked by subject, and in that folder a corresponding subjects.txt file with the corresponding subjects data to each ind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_inds = {\n",
    "'sst': [\n",
    "'correct_go',     # 0\n",
    "'incorrect_go',\n",
    "'correctlate_go',\n",
    "'noresp_go',\n",
    "'incorrectlate_go',\n",
    "'correct_stop',\n",
    "'incorrect_stop',\n",
    "'ssd_stop',\n",
    "'correct_go_vs_fixation',\n",
    "'correct_stop_vs_correct_go',  # 9\n",
    "'incorrect_stop_vs_correct_go', # 10\n",
    "'any_stop_vs_correct_go', # 11\n",
    "'correct_stop_vs_incorrect_stop', # 12\n",
    "'incorrect_go_vs_correct_go', # 13\n",
    "'incorrect_go_vs_incorrect_stop'], # 14\n",
    "\n",
    "'nback': [\n",
    "'2_back_posface',\n",
    "'2_back_neutface',\n",
    "'2_back_negface',\n",
    "'2_back_place',\n",
    "'0_back_posface',\n",
    "'0_back_neutface',\n",
    "'0_back_negface',\n",
    "'0_back_place',\n",
    "'cue',\n",
    "'0_back', # 9\n",
    "'2_back', # 10\n",
    "'place',\n",
    "'emotion',\n",
    "'2_back_vs_0_back', # 13\n",
    "'face_vs_place',\n",
    "'emotion_vs_neutface',\n",
    "'negface_vs_neutface',\n",
    "'posface_vs_neutface']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hemis(task, contrasts):\n",
    "    \n",
    "    dr = task + '_raw_data/'\n",
    "\n",
    "    lh = []\n",
    "    rh = []\n",
    "\n",
    "    for i in contrasts:\n",
    "        lh.append(nib.load(dr + 'cortical_lh_' + str(i) + '.mgz').get_fdata())\n",
    "        rh.append(nib.load(dr + 'cortical_rh_' + str(i) + '.mgz').get_fdata())\n",
    "\n",
    "    lh = np.squeeze(np.stack(lh))\n",
    "    rh = np.squeeze(np.stack(rh))\n",
    "\n",
    "    print(task, lh.shape, rh.shape)\n",
    "\n",
    "    np.save('data/lh_' + task + '.npy', lh)\n",
    "    np.save('data/rh_' + task + '.npy', rh)\n",
    "    \n",
    "    return lh, rh\n",
    "\n",
    "def load_data_subjects(task):\n",
    "    \n",
    "    with open(task + '_raw_data/subjects.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        subjects = [line.strip() for line in lines]\n",
    "        \n",
    "    return subjects\n",
    "    \n",
    "\n",
    "def make_destr_rois_csv(lh, rh, task, contrasts, contrast_inds):\n",
    "    \n",
    "    # Load subjects\n",
    "    subjects = load_data_subjects(task)\n",
    "    \n",
    "    # Load in the destr atlas\n",
    "    fs5_dr = '/usr/local/freesurfer/subjects/fsaverage5/label/'\n",
    "    destr = Existing_Surf_Individual(lh_loc = fs5_dr + 'lh.aparc.a2009s.annot',\n",
    "                                     rh_loc = fs5_dr + 'rh.aparc.a2009s.annot')\n",
    "\n",
    "    labels = io.read_annot(fs5_dr + 'lh.aparc.a2009s.annot')[2]\n",
    "    labels = [labels[i].decode(\"utf-8\") for i in range(len(labels))][1:]\n",
    "\n",
    "    # Get ROI values\n",
    "    X = get_X(lh, rh, destr.parcels_lh, destr.parcels_rh)\n",
    "\n",
    "    # Create a data frame\n",
    "    data = pd.DataFrame()\n",
    "    data['src_subject_id'] = subjects\n",
    "\n",
    "    # Put into a dataframe, and label by name\n",
    "    per_contrast = np.shape(X)[1] // len(contrasts) // 2\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(len(contrasts)):\n",
    "        \n",
    "        contrast_name = contrast_inds[task][contrasts[i]]\n",
    "        for j in range(per_contrast):\n",
    "            data[contrast_name + '.' + labels[j] + '.lh'] = X[:,cnt]\n",
    "            cnt += 1\n",
    "\n",
    "    for i in range(len(contrasts)):\n",
    "        \n",
    "        contrast_name = contrast_inds[task][contrasts[i]]\n",
    "        for j in range(per_contrast):\n",
    "            data[contrast_name + '.' + labels[j] + '.rh'] = X[:,cnt]\n",
    "            cnt += 1\n",
    "\n",
    "    # Save csv for this task\n",
    "    loc = 'data/Destr_' + task + '.csv'\n",
    "    data.to_csv(loc, index=False)\n",
    "    print('saved destr rois at:', loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data by task and contrast and save destr ROIs for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sst (7, 10242, 6204) (7, 10242, 6204)\n",
      "saved destr rois at: data/Destr_sst.csv\n",
      "nback (3, 10242, 6006) (3, 10242, 6006)\n",
      "saved destr rois at: data/Destr_nback.csv\n"
     ]
    }
   ],
   "source": [
    "sst_contrasts = [0, 9, 10, 11, 12, 13, 14]\n",
    "sst_lh, sst_rh = load_hemis('sst', sst_contrasts)\n",
    "\n",
    "make_destr_rois_csv(sst_lh, sst_rh, 'sst', sst_contrasts, contrast_inds)\n",
    "\n",
    "nback_contrasts = [9, 10, 13]\n",
    "nback_lh, nback_rh = load_hemis('nback', nback_contrasts)\n",
    "\n",
    "make_destr_rois_csv(nback_lh, nback_rh, 'nback', nback_contrasts, contrast_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is too investigate the different tasks in their own respective notebooks (nback_ml_exploration.ipynb, sst_ml_exploration.ipynb), removing severe outliers, establishing train test splits, and then determining the two ML pipelines (fast for use within search, and best)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(task):\n",
    "    \n",
    "    # Load the train test splits as established by the relevant ml_exploration scripts\n",
    "    loc = task + '_ml/'\n",
    "    \n",
    "    with open(loc + 'train_subjects.txt', 'r') as f:\n",
    "        train_subjects = f.readlines()\n",
    "        train_subjects = [s.strip() for s in train_subjects]\n",
    "        \n",
    "    with open(loc + 'test_subjects.txt', 'r') as f:\n",
    "        test_subjects = f.readlines()\n",
    "        test_subjects = [s.strip() for s in test_subjects]\n",
    "        \n",
    "    full_train_subjects = train_subjects.copy()\n",
    "        \n",
    "    # Generate a further validation set from the train set\n",
    "    train_subjects, val_subjects = train_test_split(train_subjects,\n",
    "                                                    test_size=.1,\n",
    "                                                    random_state=1)\n",
    "    \n",
    "    # Save subject splits\n",
    "    with open('data/train_subjects_' + task + '.txt', 'w') as f:\n",
    "        for subject in train_subjects:\n",
    "            f.write(subject + '\\n')\n",
    "            \n",
    "    with open('data/val_subjects_' + task + '.txt', 'w') as f:\n",
    "        for subject in val_subjects:\n",
    "            f.write(subject + '\\n')\n",
    "            \n",
    "    with open('data/test_subjects_' + task + '.txt', 'w') as f:\n",
    "        for subject in test_subjects:\n",
    "            f.write(subject + '\\n')\n",
    "            \n",
    "    with open('data/full_train_subjects_' + task + '.txt', 'w') as f:\n",
    "        for subject in full_train_subjects:\n",
    "            f.write(subject + '\\n')\n",
    "\n",
    "    # Generate split inds from the corresponding data subject list\n",
    "    subjects = load_data_subjects(task)\n",
    "\n",
    "    train_inds = [subjects.index(train_subjects[i]) for i in range(len(train_subjects))]\n",
    "    val_inds = [subjects.index(val_subjects[i]) for i in range(len(val_subjects))]\n",
    "    test_inds = [subjects.index(test_subjects[i]) for i in range(len(test_subjects))]\n",
    "    full_train_inds = [subjects.index(full_train_subjects[i]) for i in range(len(full_train_subjects))]\n",
    "    \n",
    "    print(task + ' train size:', len(train_inds))\n",
    "    print(task + ' val size:', len(val_inds))\n",
    "    print(task + ' test size:', len(test_inds))\n",
    "    print(task + ' full train size:', len(full_train_inds))\n",
    "    \n",
    "    return (train_subjects, val_subjects, test_subjects,\n",
    "            full_train_subjects, train_inds, val_inds,\n",
    "            test_inds, full_train_inds)\n",
    "\n",
    "def save_data_by_split(task, lh, rh, splits):\n",
    "    '''Splits[4,5,6,7] = train, val, test, full_train inds'''\n",
    "    \n",
    "    hemi_data = [lh, rh]\n",
    "    hemi_names = ['lh', 'rh']\n",
    "    split_names = ['train', 'val', 'test', 'full_train']\n",
    "    \n",
    "    for i in range(len(hemi_data)):\n",
    "        for j in range(len(split_names)):\n",
    "            \n",
    "            data = hemi_data[i][:,:,splits[4+j]]\n",
    "            \n",
    "            save_loc = 'data/' + split_names[j] + '_' + hemi_names[i] + '_' + task + '.pkl'\n",
    "            with open(save_loc, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "                print('saved:', save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sst train size: 4635\n",
      "sst val size: 515\n",
      "sst test size: 1000\n",
      "sst full train size: 5150\n",
      "nback train size: 4472\n",
      "nback val size: 497\n",
      "nback test size: 1000\n",
      "nback full train size: 4969\n",
      "saved: data/train_lh_sst.pkl\n",
      "saved: data/val_lh_sst.pkl\n",
      "saved: data/test_lh_sst.pkl\n",
      "saved: data/full_train_lh_sst.pkl\n",
      "saved: data/train_rh_sst.pkl\n",
      "saved: data/val_rh_sst.pkl\n",
      "saved: data/test_rh_sst.pkl\n",
      "saved: data/full_train_rh_sst.pkl\n",
      "saved: data/train_lh_nback.pkl\n",
      "saved: data/val_lh_nback.pkl\n",
      "saved: data/test_lh_nback.pkl\n",
      "saved: data/full_train_lh_nback.pkl\n",
      "saved: data/train_rh_nback.pkl\n",
      "saved: data/val_rh_nback.pkl\n",
      "saved: data/test_rh_nback.pkl\n",
      "saved: data/full_train_rh_nback.pkl\n"
     ]
    }
   ],
   "source": [
    "sst_splits = splits('sst')\n",
    "nback_splits = splits('nback')\n",
    "\n",
    "save_data_by_split('sst', sst_lh, sst_rh, sst_splits)\n",
    "save_data_by_split('nback', nback_lh, nback_rh, nback_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sst train size: 4635\n",
    "sst val size: 515\n",
    "sst test size: 1000\n",
    "nback train size: 4472\n",
    "nback val size: 497\n",
    "nback test size: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_targets_by_split(task, targets_df, target_cols, splits):\n",
    "    '''Subjects splits are splits[0,1,2,3],\n",
    "       target_cols should be a lsit'''\n",
    "    \n",
    "    targets_of_interest_df = targets_df[target_cols]\n",
    "    \n",
    "    # Save targets csvs by split\n",
    "    train_subjects_df = targets_of_interest_df.loc[splits[0]]\n",
    "    val_subjects_df = targets_of_interest_df.loc[splits[1]]\n",
    "    test_subjects_df = targets_of_interest_df.loc[splits[2]]\n",
    "    full_train_subjects_df = targets_of_interest_df.loc[splits[3]]\n",
    "    \n",
    "    save_loc = 'data/train_targets_' + task + '.csv'\n",
    "    train_subjects_df.to_csv(save_loc)\n",
    "    print('saved:', save_loc)\n",
    "    \n",
    "    save_loc = 'data/val_targets_' + task + '.csv'\n",
    "    val_subjects_df.to_csv(save_loc)\n",
    "    print('saved:', save_loc)\n",
    "    \n",
    "    save_loc = 'data/test_targets_' + task + '.csv'\n",
    "    test_subjects_df.to_csv(save_loc)\n",
    "    print('saved:', save_loc)\n",
    "    \n",
    "    save_loc = 'data/full_train_targets_' + task + '.csv'\n",
    "    full_train_subjects_df.to_csv(save_loc)\n",
    "    print('saved:', save_loc)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Save in correct formatting for usage in search\n",
    "    train_targets = []\n",
    "    for i in range(len(target_cols)):\n",
    "        train_targets.append(np.array(train_subjects_df[target_cols[i]]))\n",
    "\n",
    "    save_loc = 'data/train_targets_' + task + '.pkl'\n",
    "    with open(save_loc, 'wb') as f:\n",
    "        pickle.dump(train_targets, f)\n",
    "    print('saved:', save_loc)\n",
    "        \n",
    "    val_targets = []\n",
    "    for i in range(len(target_cols)):\n",
    "        val_targets.append(np.array(val_subjects_df[target_cols[i]]))\n",
    "        \n",
    "    save_loc = 'data/val_targets_' + task + '.pkl'\n",
    "    with open(save_loc, 'wb') as f:\n",
    "        pickle.dump(val_targets, f)\n",
    "    print('saved:', save_loc)\n",
    "        \n",
    "    test_targets = []\n",
    "    for i in range(len(target_cols)):\n",
    "        test_targets.append(np.array(test_subjects_df[target_cols[i]]))\n",
    "    \n",
    "    save_loc = 'data/test_targets_' + task + '.pkl'\n",
    "    with open(save_loc, 'wb') as f:\n",
    "        pickle.dump(test_targets, f)\n",
    "    print('saved:', save_loc)\n",
    "    \n",
    "    full_train_targets = []\n",
    "    for i in range(len(target_cols)):\n",
    "        full_train_targets.append(np.array(full_train_subjects_df[target_cols[i]]))\n",
    "\n",
    "    save_loc = 'data/full_train_targets_' + task + '.pkl'\n",
    "    with open(save_loc, 'wb') as f:\n",
    "        pickle.dump(full_train_targets, f)\n",
    "    print('saved:', save_loc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: data/train_targets_sst.csv\n",
      "saved: data/val_targets_sst.csv\n",
      "saved: data/test_targets_sst.csv\n",
      "saved: data/full_train_targets_sst.csv\n",
      "\n",
      "saved: data/train_targets_sst.pkl\n",
      "saved: data/val_targets_sst.pkl\n",
      "saved: data/test_targets_sst.pkl\n",
      "saved: data/full_train_targets_sst.pkl\n",
      "\n",
      "saved: data/train_targets_nback.csv\n",
      "saved: data/val_targets_nback.csv\n",
      "saved: data/test_targets_nback.csv\n",
      "saved: data/full_train_targets_nback.csv\n",
      "\n",
      "saved: data/train_targets_nback.pkl\n",
      "saved: data/val_targets_nback.pkl\n",
      "saved: data/test_targets_nback.pkl\n",
      "saved: data/full_train_targets_nback.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sst_targets_loc = '/mnt/Storage/To_Get/ABCD2p0NDA/abcd_sst02.txt'\n",
    "sst_targets_df = pd.read_csv(sst_targets_loc, sep='\\t', skiprows=[1], index_col='src_subject_id')\n",
    "sst_target_cols = ['tfmri_sst_all_beh_total_meanrt']\n",
    "\n",
    "save_targets_by_split('sst', sst_targets_df, sst_target_cols, sst_splits)\n",
    "\n",
    "nback_targets_loc = '/home/sage/Parcel_Search/data/nBack_target_vals.csv'\n",
    "nback_targets_df = pd.read_csv(nback_targets_loc, index_col='src_subject_id')\n",
    "nback_target_cols = ['dprime_0back', 'dprime_2back']\n",
    "\n",
    "save_targets_by_split('nback', nback_targets_df, nback_target_cols, nback_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the geo file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs5 = datasets.fetch_surf_fsaverage(mesh='fsaverage5')\n",
    "FS_AVG5_SZ = len(surface.load_surf_data(fs5['pial_left'])[0])\n",
    "\n",
    "geo = surface.load_surf_data(fs5['pial_left'])[1]\n",
    "G = nx.Graph()\n",
    "for tri in geo:\n",
    "    G.add_edge(tri[0], tri[1])\n",
    "    G.add_edge(tri[0], tri[2])\n",
    "    G.add_edge(tri[1], tri[2])\n",
    "    \n",
    "geo = []\n",
    "\n",
    "for i in range(len(G)):\n",
    "    geo.append(list(G.neighbors(i)))\n",
    "\n",
    "with open('data/geo.pkl', 'wb') as f:\n",
    "    pickle.dump(geo, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion of the setup is now complete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
